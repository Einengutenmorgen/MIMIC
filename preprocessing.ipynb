{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9e59d7",
   "metadata": {},
   "source": [
    "# Data-Prep\n",
    "\n",
    "**gefiltert wird:**\n",
    "- duplikate entferenen\n",
    "- min 100 Posts\n",
    "- min 50 Replies\n",
    "- Emoji only posts\n",
    "- nur links\n",
    "- reposts\n",
    "- weniger als 3 wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import uuid\n",
    "import re\n",
    "import emoji \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29e239c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['full_text', 'tweet_id', 'created_at', 'screen_name',\n",
       "       'original_user_id', 'retweeted_user_ID', 'collected_at', 'reply_to_id',\n",
       "       'reply_to_user', 'expandedURL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/raw/Kopie von FolloweeIDs2_tweets_df_AugustPull.csv\"\n",
    "\n",
    "dtypes = {\n",
    "    'tweet_id': str,  # Read as string for easier map keying\n",
    "    'original_user_id': str, # Assuming user IDs can be treated as strings\n",
    "    'reply_to_id': str # Read as string, will handle 'nan' or empty strings\n",
    "}\n",
    "df = pd.read_csv(path, sep=\",\", encoding=\"utf-8\", dtype=dtypes, low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b81caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_only_emojis(text):\n",
    "    \"\"\"\n",
    "    Prüft, ob ein Text nur aus Emojis und Leerzeichen besteht.\n",
    "    Diese Version ist mit neueren Versionen der 'emoji'-Bibliothek kompatibel.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    \n",
    "    # In neueren Versionen der Bibliothek wird emoji.EMOJI_DATA verwendet.\n",
    "    # Die Keys dieses Dictionaries sind die eigentlichen Emoji-Zeichen.\n",
    "    all_emojis_set = set(emoji.EMOJI_DATA.keys())\n",
    "    \n",
    "    # Entferne alle Emoji-Zeichen aus dem Text\n",
    "    text_without_emojis = ''.join(char for char in text if char not in all_emojis_set)\n",
    "    \n",
    "    # Wenn der Rest nach dem Entfernen von Leerzeichen leer ist, bestand der Text nur aus Emojis\n",
    "    return not text_without_emojis.strip()\n",
    "\n",
    "\n",
    "def is_only_link(text):\n",
    "    \"\"\"Prüft, ob ein Text nur aus URLs und Leerzeichen besteht.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n",
    "    text_without_urls = re.sub(url_pattern, '', text, flags=re.MULTILINE)\n",
    "    return not text_without_urls.strip()\n",
    "\n",
    "def analyze_url_patterns(text):\n",
    "    \"\"\"Analyze URL patterns in text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {\"has_urls\": False}\n",
    "    \n",
    "    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    \n",
    "    return {\"has_urls\": len(urls) > 0}\n",
    "\n",
    "def should_filter_context_aware(text: str, min_words: int = 5) -> bool:\n",
    "    \"\"\"Remove posts with URLs unless they have substantial context\"\"\"\n",
    "    analysis = analyze_url_patterns(text)\n",
    "    if not analysis[\"has_urls\"]:\n",
    "        return False\n",
    "    \n",
    "    # Remove if not enough meaningful text\n",
    "    text_without_urls = re.sub(r'https?://[^\\s/$.?#].[^\\s]*', '', text, flags=re.MULTILINE)\n",
    "    meaningful_words = len([word for word in text_without_urls.split() if len(word) > 2])\n",
    "    \n",
    "    return meaningful_words < min_words\n",
    "\n",
    "def contains_link(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Prüft, ob ein Text eine URL enthält.\n",
    "    Gibt True zurück, wenn mindestens eine URL gefunden wird, sonst False.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    \n",
    "    # Nutzt die bestehende Regex-Logik\n",
    "    url_pattern = r'https?://[^\\s/$.?#].[^\\s]*'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return len(urls) > 0\n",
    "\n",
    "def has_minimum_words(text, min_words=3):\n",
    "    \"\"\"\n",
    "    Check if text has at least min_words words.\n",
    "    Returns True if text has enough words, False otherwise.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return False\n",
    "    \n",
    "    # Split by whitespace and filter out empty strings\n",
    "    words = [word.strip() for word in str(text).split() if word.strip()]\n",
    "    return len(words) >= min_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b79265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/raw/Kopie von FolloweeIDs2_tweets_df_AugustPull.csv\n",
      "Spalten im DataFrame: ['full_text', 'tweet_id', 'created_at', 'screen_name', 'original_user_id', 'retweeted_user_ID', 'collected_at', 'reply_to_id', 'reply_to_user', 'expandedURL']\n",
      "Anfängliche Anzahl Zeilen: 7790741, nach Duplikaten entfernen: 6900898\n",
      "Anzahl Zeilen nach dem Entfernen von Retweets: 4728900 (reduziert um 3061841)\n",
      "Anzahl Zeilen nach Entfernen von Nur-Emoji-Posts: 4727989  (reduziert um 3062752)\n",
      "Anzahl Zeilen nach Entfernen von Nur-Link-Posts: 2169889  (reduziert um 5620852)\n",
      "Anzahl Zeilen nach Entfernen von Posts mit weniger als 3 Wörtern: 2023786 (reduziert um 5766955)\n",
      "Filtering eligible users...\n",
      "Anzahl geeigneter Nutzer gefunden: 25\n",
      "Processing user: 1.0679687024008764e+18\n",
      "  Successfully processed and saved data for user 1.0679687024008764e+18\n",
      "Processing user: 1.470492384274309e+18\n",
      "  Successfully processed and saved data for user 1.470492384274309e+18\n",
      "Processing user: 38283342.0\n",
      "  Successfully processed and saved data for user 38283342.0\n",
      "Processing user: 527526077.0\n",
      "  Successfully processed and saved data for user 527526077.0\n",
      "Processing user: 2570955294.0\n",
      "  Successfully processed and saved data for user 2570955294.0\n",
      "Processing user: 182179968.0\n",
      "  Successfully processed and saved data for user 182179968.0\n",
      "Processing user: 409824643.0\n",
      "  Successfully processed and saved data for user 409824643.0\n",
      "Processing user: 1.26461714332638e+18\n",
      "  Successfully processed and saved data for user 1.26461714332638e+18\n",
      "Processing user: 1.2133059545208463e+18\n",
      "  Successfully processed and saved data for user 1.2133059545208463e+18\n",
      "Processing user: 3138390312.0\n",
      "  Successfully processed and saved data for user 3138390312.0\n",
      "Processing user: 1390669327.0\n",
      "  Successfully processed and saved data for user 1390669327.0\n",
      "Processing user: 1.4045329932266865e+18\n",
      "  Successfully processed and saved data for user 1.4045329932266865e+18\n",
      "Processing user: 17813513.0\n",
      "  Successfully processed and saved data for user 17813513.0\n",
      "Processing user: 1.063839405977338e+18\n",
      "  Successfully processed and saved data for user 1.063839405977338e+18\n",
      "Processing user: 8.840833612953354e+17\n",
      "  Successfully processed and saved data for user 8.840833612953354e+17\n",
      "Processing user: 1.5743066908578488e+18\n",
      "  Successfully processed and saved data for user 1.5743066908578488e+18\n",
      "Processing user: 1.168793622248067e+18\n",
      "  Successfully processed and saved data for user 1.168793622248067e+18\n",
      "Processing user: 1.1782932009073132e+18\n",
      "  Successfully processed and saved data for user 1.1782932009073132e+18\n",
      "Processing user: 302225782.0\n",
      "  Successfully processed and saved data for user 302225782.0\n",
      "Processing user: 16862150.0\n",
      "  Successfully processed and saved data for user 16862150.0\n",
      "Processing user: 157516153.0\n",
      "  Successfully processed and saved data for user 157516153.0\n",
      "Processing user: 1.05272263315268e+18\n",
      "  Successfully processed and saved data for user 1.05272263315268e+18\n",
      "Processing user: 1362828098.0\n",
      "  Successfully processed and saved data for user 1362828098.0\n",
      "Processing user: 3225632674.0\n",
      "  Successfully processed and saved data for user 3225632674.0\n",
      "Processing user: 1.4204222245914296e+18\n",
      "  Successfully processed and saved data for user 1.4204222245914296e+18\n",
      "\n",
      "Finished processing. Total users processed and saved: 25\n",
      "User statistics saved to: output_directory_v2/user_stats.json\n",
      "List of eligible users saved to: output_directory_v2/eligible_users.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "TARGET_POSTS_PER_SET = 100  # Total posts in each history/holdout set\n",
    "TARGET_REPLIES_PER_SET = 50   # Number of replies in each set\n",
    "TARGET_ORIGINALS_PER_SET = TARGET_POSTS_PER_SET - TARGET_REPLIES_PER_SET\n",
    "\n",
    "# --- File Path ---\n",
    "path = \"data/raw/Kopie von FolloweeIDs2_tweets_df_AugustPull.csv\"\n",
    "\n",
    "# --- Main Script ---\n",
    "def process_tweets(csv_path):\n",
    "    print(f\"Loading data from: {csv_path}\")\n",
    "\n",
    "    # Specify dtypes for key ID columns and others prone to mixed types\n",
    "    # Reading IDs as strings simplifies handling and map keying.\n",
    "    dtypes = {\n",
    "        'tweet_id': str,\n",
    "        'original_user_id': str,\n",
    "        'reply_to_id': str,\n",
    "        'screen_name': str, # Was in DtypeWarning\n",
    "        'created_at': str,  # Was in DtypeWarning, consider parse_dates if needed\n",
    "        'collected_at': str,# Was in DtypeWarning\n",
    "        'expandedURL': str  # Was in DtypeWarning\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path, \n",
    "            sep=\",\", \n",
    "            encoding=\"utf-8\", \n",
    "            dtype=dtypes, \n",
    "            low_memory=False, # Safer with complex CSVs / mixed types\n",
    "            on_bad_lines='skip'\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Spalten im DataFrame:\", df.columns.tolist())\n",
    "\n",
    "    # --- Data Cleaning and Preparation ---\n",
    "    # Clean 'reply_to_id' to use pd.NA for missing values for consistent checking\n",
    "    # common string representations of NaN/None when read as str dtype\n",
    "    df['reply_to_id'] = df['reply_to_id'].replace({'nan': pd.NA, '': pd.NA, None: pd.NA, 'None': pd.NA})\n",
    "    # Ensure tweet_id is string and suitable for map keys (already str by dtype)\n",
    "    df['tweet_id'] = df['tweet_id'].astype(str)\n",
    "\n",
    "\n",
    "    # Remove duplicates based on 'tweet_id'\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(subset=['tweet_id'], inplace=True, keep='first')\n",
    "    print(f\"Anfängliche Anzahl Zeilen: {initial_rows}, nach Duplikaten entfernen: {len(df)}\")\n",
    "    df = df[df['retweeted_user_ID'].isna()]\n",
    "    print(f\"Anzahl Zeilen nach dem Entfernen von Retweets: {len(df)} (reduziert um {initial_rows-len(df)})\")\n",
    "    # 2. Nur-Emoji-Posts entfernen\n",
    "    # Wir wenden die Hilfsfunktion auf die 'full_text'-Spalte an\n",
    "    df = df[~df['full_text'].apply(is_only_emojis)]\n",
    "    print(f\"Anzahl Zeilen nach Entfernen von Nur-Emoji-Posts: {len(df)}  (reduziert um {initial_rows-len(df)})\")\n",
    "\n",
    "    # 3. Context-aware URL filtering - Replace the old is_only_link function\n",
    "    # Remove posts with URLs unless they have substantial context\n",
    "    #df = df[~df['full_text'].apply(is_only_link)]\n",
    "    df = df[~df['full_text'].apply(contains_link)]\n",
    "    print(f\"Anzahl Zeilen nach Entfernen von Nur-Link-Posts: {len(df)}  (reduziert um {initial_rows-len(df)})\")\n",
    "\n",
    "    #4. Remove posts with less than 3 words\n",
    "    df = df[df['full_text'].apply(has_minimum_words)]\n",
    "    print(f\"Anzahl Zeilen nach Entfernen von Posts mit weniger als 3 Wörtern: {len(df)} (reduziert um {initial_rows-len(df)})\")\n",
    "\n",
    "\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty after removing duplicates. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Create a lookup map for tweet details (used for 'previous_message')\n",
    "    # This map uses the string version of tweet_id as keys.\n",
    "    tweet_details_map = df.set_index('tweet_id')[['created_at', 'screen_name', 'full_text']].to_dict(orient='index')\n",
    "\n",
    "    # --- User Filtering ---\n",
    "    # Ensure 'original_user_id' is not null before counting\n",
    "    df_valid_users = df[df['original_user_id'].notna()]\n",
    "    original_tweet_counts = df_valid_users['original_user_id'].value_counts()\n",
    "    \n",
    "    replies_df_all = df_valid_users[df_valid_users['reply_to_id'].notnull()]\n",
    "    reply_counts = replies_df_all['original_user_id'].value_counts()\n",
    "\n",
    "    eligible_users = []\n",
    "    user_stats = []\n",
    "\n",
    "    print(\"Filtering eligible users...\")\n",
    "    for user_id_str in original_tweet_counts.index:\n",
    "        total_posts = original_tweet_counts.get(user_id_str, 0)\n",
    "        num_replies = reply_counts.get(user_id_str, 0)\n",
    "        num_originals = total_posts - num_replies\n",
    "\n",
    "        if num_replies >= (2 * TARGET_REPLIES_PER_SET) and \\\n",
    "           num_originals >= (2 * TARGET_ORIGINALS_PER_SET) and \\\n",
    "           total_posts >= (2 * TARGET_POSTS_PER_SET): # Ensure enough total posts for two sets\n",
    "            eligible_users.append(user_id_str)\n",
    "            user_stats.append({\n",
    "                \"user_id\": user_id_str,\n",
    "                \"total_tweets_by_user\": int(total_posts), # Renamed for clarity\n",
    "                \"replies_by_user\": int(num_replies),\n",
    "                \"original_tweets_by_user\": int(num_originals) # Non-replies\n",
    "            })\n",
    "    \n",
    "    print(f\"Anzahl geeigneter Nutzer gefunden: {len(eligible_users)}\")\n",
    "    if not eligible_users:\n",
    "        print(\"Keine geeigneten Nutzer gefunden basierend auf den Kriterien. Beende Skript.\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare Output Directory ---\n",
    "    output_dir = Path(\"data/filtered_users_without_links\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Helper function to get previous message details ---\n",
    "    def get_previous_message_details(reply_row_series, tdm):\n",
    "        original_tweet_id = reply_row_series['reply_to_id'] # This should be a string ID now\n",
    "        if pd.notnull(original_tweet_id):\n",
    "            # original_tweet_id is already a string from dtype and cleaning\n",
    "            message_data = tdm.get(original_tweet_id)\n",
    "            if message_data:\n",
    "                # Ensure screen_name is handled if it's missing or NaN from the source\n",
    "                screen_name_val = message_data.get('screen_name')\n",
    "                formatted_screen_name = str(screen_name_val) if pd.notnull(screen_name_val) else ''\n",
    "                \n",
    "                # Format as a single string: \"Date, Author: Message\"\n",
    "                created_at = message_data.get('created_at', '')\n",
    "                full_text = message_data.get('full_text', '')\n",
    "                \n",
    "                formatted_message = f\"{created_at}, {formatted_screen_name}: {full_text}\"\n",
    "                return formatted_message\n",
    "        return None\n",
    "\n",
    "    # --- Process Each Eligible User ---\n",
    "    processed_user_count = 0\n",
    "    for user_id_str in eligible_users:\n",
    "        print(f\"Processing user: {user_id_str}\")\n",
    "        user_df = df[df['original_user_id'] == user_id_str].copy()\n",
    "        \n",
    "        # Shuffle all user's tweets once\n",
    "        user_df = user_df.sample(frac=1, random_state=42)\n",
    "\n",
    "        all_user_replies = user_df[user_df['reply_to_id'].notnull()].copy()\n",
    "        all_user_originals = user_df[user_df['reply_to_id'].isnull()].copy()\n",
    "\n",
    "        # Check again if enough material after splitting (should be guaranteed by initial filter)\n",
    "        if not (len(all_user_replies) >= (2 * TARGET_REPLIES_PER_SET) and \\\n",
    "                len(all_user_originals) >= (2 * TARGET_ORIGINALS_PER_SET)):\n",
    "            print(f\"  Skipping user {user_id_str}: Insufficient replies/originals after splitting (unexpected).\")\n",
    "            continue\n",
    "            \n",
    "        # Create Holdout Set\n",
    "        holdout_replies = all_user_replies.head(TARGET_REPLIES_PER_SET)\n",
    "        holdout_originals = all_user_originals.head(TARGET_ORIGINALS_PER_SET)\n",
    "        holdout_df = pd.concat([holdout_replies, holdout_originals]).sample(frac=1, random_state=42) # Shuffle combined parts\n",
    "\n",
    "        # Create History Set (from parts not used in holdout)\n",
    "        history_replies = all_user_replies.iloc[TARGET_REPLIES_PER_SET : 2 * TARGET_REPLIES_PER_SET]\n",
    "        history_originals = all_user_originals.iloc[TARGET_ORIGINALS_PER_SET : 2 * TARGET_ORIGINALS_PER_SET]\n",
    "        history_df = pd.concat([history_replies, history_originals]).sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "        # Validate set composition\n",
    "        valid_sets = True\n",
    "        if not (len(holdout_df) == TARGET_POSTS_PER_SET and \\\n",
    "                len(holdout_df[holdout_df['reply_to_id'].notnull()]) == TARGET_REPLIES_PER_SET):\n",
    "            print(f\"  Skipping user {user_id_str}: Holdout set composition incorrect. Size: {len(holdout_df)}, Replies: {len(holdout_df[holdout_df['reply_to_id'].notnull()])}\")\n",
    "            valid_sets = False\n",
    "        \n",
    "        if not (len(history_df) == TARGET_POSTS_PER_SET and \\\n",
    "                len(history_df[history_df['reply_to_id'].notnull()]) == TARGET_REPLIES_PER_SET):\n",
    "            print(f\"  Skipping user {user_id_str}: History set composition incorrect. Size: {len(history_df)}, Replies: {len(history_df[history_df['reply_to_id'].notnull()])}\")\n",
    "            valid_sets = False\n",
    "        \n",
    "        if not valid_sets:\n",
    "            continue\n",
    "\n",
    "        # Add 'previous_message' to replies in both sets\n",
    "        for current_set_df in [history_df, holdout_df]:\n",
    "            current_set_df['previous_message'] = None # Initialize column\n",
    "            reply_mask = current_set_df['reply_to_id'].notnull()\n",
    "            \n",
    "            # Ensure apply is only on rows where reply_mask is True and avoid issues with empty slices\n",
    "            if reply_mask.any():\n",
    "                 current_set_df.loc[reply_mask, 'previous_message'] = current_set_df[reply_mask].apply(\n",
    "                    lambda row: get_previous_message_details(row, tweet_details_map), axis=1\n",
    "                )\n",
    "        \n",
    "        # Save user's data to JSONL\n",
    "        output_path = output_dir / f\"{user_id_str}.jsonl\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                # History set\n",
    "                history_records = history_df.to_dict(orient=\"records\")\n",
    "                json.dump({\"user_id\": user_id_str, \"set\": \"history\", \"tweets\": history_records}, f)\n",
    "                f.write('\\n')\n",
    "                \n",
    "                # Holdout set\n",
    "                holdout_records = holdout_df.to_dict(orient=\"records\")\n",
    "                json.dump({\"user_id\": user_id_str, \"set\": \"holdout\", \"tweets\": holdout_records}, f)\n",
    "                f.write('\\n')\n",
    "            processed_user_count += 1\n",
    "            print(f\"  Successfully processed and saved data for user {user_id_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving data for user {user_id_str}: {e}\")\n",
    "\n",
    "    print(f\"\\nFinished processing. Total users processed and saved: {processed_user_count}\")\n",
    "\n",
    "    # --- Save Global Metadata ---\n",
    "    meta_dir = Path(\"output_directory_v2\")\n",
    "    meta_dir.mkdir(parents=True, exist_ok=True) # Ensure base output_directory exists\n",
    "\n",
    "    final_user_stats_path = meta_dir / \"user_stats.json\"\n",
    "    with open(final_user_stats_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(user_stats, f, indent=4)\n",
    "    print(f\"User statistics saved to: {final_user_stats_path}\")\n",
    "\n",
    "    # Save only the list of users for whom files were actually created (or attempted successfully)\n",
    "    # For simplicity, we are saving the 'eligible_users' list based on initial criteria.\n",
    "    # A more precise list would be users for whom files were actually written.\n",
    "    eligible_users_path = meta_dir / \"eligible_users.json\"\n",
    "    with open(eligible_users_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(eligible_users, f, indent=4) # This is the list of users meeting the criteria for set creation\n",
    "    print(f\"List of eligible users saved to: {eligible_users_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_tweets(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
