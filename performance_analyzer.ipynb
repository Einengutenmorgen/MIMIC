{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook analyzes the performance of different experiment runs based on a common prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from round_analysis import RoundAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find Matching Run IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_run_ids(users_directory, prefix):\n",
    "    \"\"\"Find all run_ids in the user files that start with a given prefix.\"\"\"\n",
    "    run_ids = set()\n",
    "    user_files = [f for f in os.listdir(users_directory) if f.endswith('.jsonl')]\n",
    "    \n",
    "    for user_file in user_files:\n",
    "        user_file_path = os.path.join(users_directory, user_file)\n",
    "        try:\n",
    "            with open(user_file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) > 2:\n",
    "                    runs_content = json.loads(lines[2].strip())\n",
    "                    for run in runs_content.get('runs', []):\n",
    "                        run_id = run.get('run_id')\n",
    "                        if run_id and run_id.startswith(prefix):\n",
    "                            run_ids.add(run_id)\n",
    "        except (json.JSONDecodeError, IndexError, FileNotFoundError) as e:\n",
    "            print(f\"Could not process file {user_file}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return sorted(list(run_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Extract Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(df):\n",
    "    \"\"\"Extract evaluation metrics from the raw data format into a flat DataFrame.\"\"\"\n",
    "    extracted_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            base_row = {\n",
    "                'user_id': row['user_id'],\n",
    "                'round': row['round'],\n",
    "                'run_id': row['run_id'],\n",
    "                'timestamp': row['timestamp']\n",
    "            }\n",
    "\n",
    "            # Extract from statistics dictionary\n",
    "            if isinstance(row.get('statistics'), dict):\n",
    "                stats = row['statistics']\n",
    "                base_row['mean_combined_score'] = stats.get('mean_combined_score', 0)\n",
    "                base_row['std_combined_score'] = stats.get('std_combined_score', 0)\n",
    "                base_row['best_score'] = stats.get('best_score', 0)\n",
    "                base_row['worst_score'] = stats.get('worst_score', 0)\n",
    "\n",
    "            # Extract from overall dictionary\n",
    "            if isinstance(row.get('overall'), dict):\n",
    "                overall = row['overall']\n",
    "                base_row['overall_score'] = overall.get('score', 0)\n",
    "                if isinstance(overall.get('rouge'), dict):\n",
    "                    base_row['overall_rouge1'] = overall['rouge'].get('rouge1', 0)\n",
    "                if isinstance(overall.get('bleu'), dict):\n",
    "                    base_row['overall_bleu'] = overall['bleu'].get('bleu', 0)\n",
    "\n",
    "            # Extract from individual scores list\n",
    "            if isinstance(row.get('individual_scores'), list):\n",
    "                scores = [s.get('combined_score', 0) for s in row['individual_scores'] if isinstance(s, dict)]\n",
    "                if scores:\n",
    "                    base_row['individual_mean_combined'] = np.mean(scores)\n",
    "                    base_row['individual_max_combined'] = np.max(scores)\n",
    "                    base_row['individual_min_combined'] = np.min(scores)\n",
    "\n",
    "            extracted_rows.append(base_row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row for user {row.get('user_id')}, run {row.get('run_id')}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return pd.DataFrame(extracted_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_data(users_directory, run_ids):\n",
    "    \"\"\"Load data for the given run_ids and extract metrics.\"\"\"\n",
    "    print(\"\\nLoading and processing data...\")\n",
    "    analyzer = RoundAnalyzer(users_directory=users_directory)\n",
    "    \n",
    "    # Get the raw data for all users\n",
    "    df_raw = analyzer.analyze_all_users()\n",
    "    \n",
    "    if df_raw.empty:\n",
    "        print(\"No data loaded from RoundAnalyzer.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Filter for the runs we are interested in\n",
    "    df_filtered = df_raw[df_raw['run_id'].isin(run_ids)].copy()\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data found for the specified run IDs.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"Found {len(df_filtered)} records for the specified runs.\")\n",
    "    \n",
    "    # Extract metrics\n",
    "    df_metrics = extract_metrics(df_filtered)\n",
    "    print(f\"Successfully extracted metrics into {len(df_metrics)} records.\")\n",
    "    \n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summarize and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_statistics(df_metrics):\n",
    "    \"\"\"Calculate summary statistics for each run.\"\"\"\n",
    "    if df_metrics.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Define the metrics to aggregate\n",
    "    metrics_to_agg = [\n",
    "        'mean_combined_score', 'best_score', 'worst_score', \n",
    "        'overall_score', 'overall_rouge1', 'overall_bleu',\n",
    "        'individual_mean_combined'\n",
    "    ]\n",
    "    \n",
    "    # Filter out metrics that are not in the dataframe\n",
    "    existing_metrics = [m for m in metrics_to_agg if m in df_metrics.columns]\n",
    "    \n",
    "    summary = df_metrics.groupby('run_id')[existing_metrics].agg(['mean', 'std', 'count', 'min', 'max']).round(4)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(df_metrics, metric='mean_combined_score'):\n",
    "    \"\"\"Create a boxplot to compare a metric across different runs.\"\"\"\n",
    "    if df_metrics.empty or metric not in df_metrics.columns:\n",
    "        print(f\"Cannot plot '{metric}' as it's not available in the data.\")\n",
    "        return\n",
    "        \n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    sns.boxplot(data=df_metrics, x='run_id', y=metric, ax=ax)\n",
    "    sns.stripplot(data=df_metrics, x='run_id', y=metric, ax=ax, color='black', alpha=0.3, size=4)\n",
    "    \n",
    "    ax.set_title(f'Comparison of {metric.replace(\"_\", \" \").title()} Across Runs', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Run ID', fontsize=12)\n",
    "    ax.set_ylabel(metric.replace(\"_\", \" \").title(), fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(run_name_prefix, users_directory='data/filtered_users'):\n",
    "    \"\"\"\n",
    "    Analyzes performance for all runs matching a given prefix.\n",
    "    \n",
    "    :param run_name_prefix: The prefix of the run names to analyze.\n",
    "    :param users_directory: Path to the directory containing user .jsonl files.\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing runs with prefix: {run_name_prefix}\\n\")\n",
    "    \n",
    "    # 1. Find all runs matching the prefix\n",
    "    matching_run_ids = find_run_ids(users_directory, run_name_prefix)\n",
    "    \n",
    "    if not matching_run_ids:\n",
    "        print(f\"No runs found with prefix '{run_name_prefix}'.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(matching_run_ids)} matching runs:\")\n",
    "    for run_id in matching_run_ids:\n",
    "        print(f\"  - {run_id}\")\n",
    "        \n",
    "    # 2. Load data and extract metrics\n",
    "    df_metrics = load_and_extract_data(users_directory, matching_run_ids)\n",
    "    \n",
    "    if df_metrics.empty:\n",
    "        print(\"Analysis halted as no metrics could be extracted.\")\n",
    "        return\n",
    "        \n",
    "    # 3. Aggregate and compare results\n",
    "    summary_stats = get_summary_statistics(df_metrics)\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # 4. Plot visualizations\n",
    "    print(\"\\n--- Visualizations ---\")\n",
    "    plot_metric_comparison(df_metrics, metric='mean_combined_score')\n",
    "    plot_metric_comparison(df_metrics, metric='best_score')\n",
    "    plot_metric_comparison(df_metrics, metric='overall_rouge1')\n",
    "    \n",
    "    print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    RUN_PREFIX = \"2025_07_09\"  # <<< CHANGE THIS TO YOUR RUN PREFIX\n",
    "    USERS_DIR = \"data/filtered_users\"\n",
    "    \n",
    "    analyze_performance(RUN_PREFIX, USERS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}